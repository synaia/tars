{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9e60719",
   "metadata": {},
   "source": [
    "### Conversational AI - Research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "62681546-d45d-429c-91b5-4c83c30bd7e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "62724.11s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pgvector in /Users/beltre.wilton/miniforge3/envs/tars_env/lib/python3.10/site-packages (0.2.5)\n",
      "Requirement already satisfied: psycopg2 in /Users/beltre.wilton/miniforge3/envs/tars_env/lib/python3.10/site-packages (2.9.9)\n",
      "Requirement already satisfied: einops in /Users/beltre.wilton/miniforge3/envs/tars_env/lib/python3.10/site-packages (0.8.0)\n",
      "Requirement already satisfied: numpy in /Users/beltre.wilton/miniforge3/envs/tars_env/lib/python3.10/site-packages (from pgvector) (1.26.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "62731.62s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in /Users/beltre.wilton/miniforge3/envs/tars_env/lib/python3.10/site-packages (0.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pgvector psycopg2 einops\n",
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "3083dba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import dspy\n",
    "from dspy.functional import TypedPredictor\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "from transitions import Machine\n",
    "from dotenv import dotenv_values\n",
    "from rich import print\n",
    "\n",
    "\n",
    "secret = dotenv_values('../../.secret')\n",
    "\n",
    "gpt3_5_turbo_recruiter  = dspy.OpenAI(\n",
    "    model='gpt-3.5-turbo-0125',\n",
    "    api_key=secret['OPEN_AI_API_KEY'],\n",
    "    max_tokens=4096\n",
    ")\n",
    "\n",
    "gpt4o  = dspy.OpenAI(\n",
    "    model='gpt-4o',\n",
    "    api_key=secret['OPEN_AI_API_KEY'],\n",
    "    max_tokens=4096\n",
    ")\n",
    "\n",
    "\n",
    "llama3 = dspy.GROQ(\n",
    "    model='llama3-70b-8192',\n",
    "    api_key=secret['GROQ_API_KEY'],\n",
    "    max_tokens=4096*2\n",
    ")\n",
    "\n",
    "dspy.settings.configure(lm=llama3)\n",
    "\n",
    "\n",
    "def get_device():\n",
    "    device = \"cuda\" \\\n",
    "        if torch.cuda.is_available() \\\n",
    "        else \"mps\" if torch.backends.mps.is_available() \\\n",
    "        else \"cpu\"\n",
    "    if device == \"mps\":\n",
    "        import os\n",
    "        os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "        os.environ[\"PYTORCH_MPS_HIGH_WATERMARK_RATIO\"] = \"0.0\"\n",
    "    return torch.device(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "d9473bc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:transformers_modules.nomic-ai.nomic-bert-2048.e55a7d4324f65581af5f483e830b80f34680e8ff.modeling_hf_nomic_bert:<All keys matched successfully>\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "\n",
    "tokenizer_embed = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "model_embed = AutoModel.from_pretrained('nomic-ai/nomic-embed-text-v1.5', trust_remote_code=True, safe_serialization=True)\n",
    "model_embed.eval()\n",
    "\n",
    "\n",
    "def embedd(text: str):\n",
    "    def mean_pooling(model_output, attention_mask):\n",
    "        token_embeddings = model_output[0]\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "\n",
    "    encoded_input = tokenizer_embed(text, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "    # + matryoshka_dim = 512\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model_output = model_embed(**encoded_input)\n",
    "\n",
    "    embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "    # + embeddings = F.layer_norm(embeddings, normalized_shape=(embeddings.shape[1],))\n",
    "    # + embeddings = embeddings[:, :matryoshka_dim]\n",
    "    embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "\n",
    "    return np.array(embeddings)[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "5d71bc8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "from psycopg2.extras import execute_values\n",
    "from pgvector.psycopg2 import register_vector\n",
    "\n",
    "def persona_hub(persona_desc: str, subset: str) -> None:\n",
    "    try:\n",
    "        connection = psycopg2.connect(user=\"drfadul\",\n",
    "                                    password=\"dROG@dijoFadul\",\n",
    "                                    host=\"localhost\",\n",
    "                                    port=\"5432\",\n",
    "                                    database=\"synaia\")\n",
    "        \n",
    "        register_vector(connection)\n",
    "        cursor = connection.cursor()\n",
    "        data = [\n",
    "            (subset, persona_desc, embedd(text=persona_desc))\n",
    "        ]\n",
    "\n",
    "        execute_values(cursor, \"INSERT INTO persona_hub (subset, persona_desc, persona_embedding) VALUES %s\", data)\n",
    "        connection.commit()\n",
    "\n",
    "\n",
    "    except (Exception, psycopg2.Error) as error:\n",
    "        print(\"Error while INSERTING data from PostgreSQL\", error)\n",
    "\n",
    "    finally:\n",
    "        if connection:\n",
    "            cursor.close()\n",
    "            connection.close()\n",
    "\n",
    "\n",
    "\n",
    "def retrieve_persona(text: str, K: int, negative_similarity: bool = False) -> list:\n",
    "    personas = []\n",
    "    try:\n",
    "        connection = psycopg2.connect(user=\"drfadul\",\n",
    "                                    password=\"dROG@dijoFadul\",\n",
    "                                    host=\"localhost\",\n",
    "                                    port=\"5432\",\n",
    "                                    database=\"synaia\")\n",
    "        register_vector(connection)\n",
    "        cursor = connection.cursor()\n",
    "        text_embedd = embedd(text=text)\n",
    "        data = (text_embedd, text_embedd, K)\n",
    "    \n",
    "        persona_hub_query = \"SELECT subset, persona_desc, 1 - (persona_embedding <=> %s::vector) AS similarity FROM persona_hub ORDER BY persona_embedding <=> %s::vector LIMIT %s\"\n",
    "        if negative_similarity:\n",
    "            persona_hub_query = \"SELECT subset, persona_desc, 1 - (persona_embedding <=> %s::vector) AS similarity FROM persona_hub ORDER BY persona_embedding <=> %s::vector DESC LIMIT %s\"\n",
    "\n",
    "        cursor.execute(persona_hub_query, data)\n",
    "        records = cursor.fetchall()\n",
    "\n",
    "        for row in records:\n",
    "            personas.append(\n",
    "                (row[1], row[2])\n",
    "            )\n",
    "        \n",
    "        return personas\n",
    "\n",
    "    except (Exception, psycopg2.Error) as error:\n",
    "        print(\"\\x1b[1;31m Error while fetching data from PostgreSQL \\x1b[1;31m\", error)\n",
    "\n",
    "    finally:\n",
    "        # closing database connection.\n",
    "        if connection:\n",
    "            cursor.close()\n",
    "            connection.close()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "83a6bd08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Total number of input personas: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">200000</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Total number of input personas: \u001b[1;36m200000\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "sample_size = 200_000\n",
    "\n",
    "# Load the dataset\n",
    "persona_dataset = load_dataset(\"proj-persona/PersonaHub\", data_files=\"persona.jsonl\")['train']\n",
    "persona_dataset = persona_dataset[:sample_size]\n",
    "print(f\"Total number of input personas: {len(persona_dataset['persona'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "2f1a71dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    for persona in tqdm(persona_dataset['persona']):\n",
    "        persona = persona.strip()\n",
    "        persona_hub(persona, \"persona\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "26403163",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AlbertTokenizer, AlbertForSequenceClassification\n",
    "import random\n",
    "\n",
    "# Define the model name (the name should correspond to the fine-tuned model you want to use)\n",
    "model_name = \"JanSt/albert-base-v2_mbti-classification\"\n",
    "device = get_device()\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = AlbertTokenizer.from_pretrained(model_name)\n",
    "model = AlbertForSequenceClassification.from_pretrained(model_name)\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "def get_personality_indicator(persona: str):\n",
    "    # Tokenize the text\n",
    "    inputs = tokenizer(persona, return_tensors='pt').to(device)\n",
    "\n",
    "    # Perform inference\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    # Get the predicted class\n",
    "    logits = outputs.logits\n",
    "    probabilities = F.softmax(logits, dim=1)\n",
    "    probabilities_list = probabilities.squeeze().tolist()\n",
    "    probabilities_list = {model.config.id2label[i]: p for i, p in enumerate(probabilities_list)}\n",
    "    p_type = max(probabilities_list, key=lambda x: probabilities_list[x])\n",
    "\n",
    "    return p_type, probabilities_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "ca0e05c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "WARNING:huggingface_hub.repocard:Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Total number of myers-briggs records: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8675</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Total number of myers-briggs records: \u001b[1;36m8675\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "mb_dataset = load_dataset(\"kl08/myers-briggs-type-indicator\")['train']\n",
    "print(f\"Total number of myers-briggs records: {len(mb_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "2fed0baf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAADMAAAAQCAYAAAC7mUeyAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy80BEi2AAAACXBIWXMAABJ0AAASdAHeZh94AAACcElEQVR4nN3XTcgWVRQH8N+rgl8EhQmKC8VwYZGpgSiKKQhiSqloK8M26UYQUcqFcDjtciG6TBTfhe4EW+QHKLkQE4XEhShkuLHIItsEIoK+LeZOjOPz+H48z8oDw50599z7P/85H3dmYGhoyOsiE9qKzFyP3XgX0/AHfsahiLjWst2Cj7AQH+ANnIqIba8CHSXGFzgxDI/nETF+XGvht/gBi3EBR3ATn+JqZradPIBdhczvwwCOFeMWssv1Y7E5TyMymTkD+/AnFkTEX4251WXhNzjZANqD3/CrKkKXhyEyaoyIuFUIddqvjuJRaEZmdnm+3gQpG17Gv5je1kfEvYgYaeGNGqObZOb7WKrKiLNtMvfwFEsy8+3WwpWqerg0Qqe7ST8xdpTxeEQ8o5FmEfFPZn6NQ7iTmd/jEd7BJ7iInWPn0T+MzJyMbXiGY7X+hQYQEYexWUXyS+zHVjzAYDs1xkioHxif4U1ciIgHtbLdzb7CaQyq3tZUfIj7OJWZB3uj0jeMOsW+ayoH6kMzM1eputGZiNjccmAKfsFMzIuI+x2crNd3PWd6xSh27+G2qovOqeuFFyOzoYwvtdeIeIwbxX5RJ5ARSj8wXir8WppkJpaxW2us9U9f5e0w0hNGZk7C56rCP96eb5K5UsYdmTmrtck6LMcT/DQitztLrxhb8RbONwu/lua32WlVj1+Du5l5Bg8xX5UeA9gfEY8aDmzExvI4o4zLMnOw3P8dEft6wWhJnWJHO03+H5mIeI6PVZ8od7AJe1Wn7DmsjYgjrfULsb1ca4tubkO3pWk8RgyQmfOxQlX45zrZDLxOvwD/AQKtKEY3SXSlAAAAAElFTkSuQmCC",
      "text/latex": [
       "$\\displaystyle 8187$"
      ],
      "text/plain": [
       "8187"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "from rich import print\n",
    "\n",
    "rdm = random.randint(0, len(mb_dataset))\n",
    "min_len_post = 150 # chars\n",
    "min_posts = 10\n",
    "N = len(mb_dataset)\n",
    "post_list = []\n",
    "\n",
    "for p_type, posts in zip(mb_dataset['type'][:N], mb_dataset['posts'][:N]):\n",
    "    posts = posts.split(\"|||\")\n",
    "    posts = {post for post in posts if \"http\" not in post and \"www\" not in post and len(post) > min_len_post}\n",
    "    if len(posts) > min_posts:\n",
    "        post_list.append(\n",
    "            {\n",
    "                \"type\": p_type,\n",
    "                \"posts\": posts\n",
    "            }\n",
    "        )\n",
    "\n",
    "len(post_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "31564d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 100\n",
    "key_words = [\n",
    "    ['Young', 'Modern', 'Joyful', 'Outspoken', 'Inexperienced', \"Open-minded\", 'Smart', \"Change-resistant\", 'selfish', 'Honest', 'Relaxed', \"Job\"]\n",
    "]\n",
    "\n",
    "__persona_applicant = []\n",
    "for kw in [\" \".join(k).strip() for k in key_words]:\n",
    "    retrieved = retrieve_persona(text=kw, K=K)\n",
    "    for r in retrieved:\n",
    "        __persona_applicant.append(r[0])\n",
    "\n",
    "\n",
    "\n",
    "key_words = [\n",
    "    [\"friendly\", \"knowledable\", \"professional\", \"women\", \"talkative\", \"Work\", \"Job\"],\n",
    "    # [\"racing\", \"adventurer\", \"rich\", \"job\"],\n",
    "]\n",
    "\n",
    "__persona_recruitment = []\n",
    "for kw in [\" \".join(k).strip() for k in key_words]:\n",
    "    retrieved = retrieve_persona(text=kw, K=K)\n",
    "    for r in retrieved:\n",
    "        __persona_recruitment.append(r[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "381cfc0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SYNTH DATA $ 💉 COOKING ☕️:   0%|                                                    | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SYNTH DATA $ 💉 COOKING ☕️: 100%|██████████████████████████████████████████| 100/100 [59:57<00:00, 35.97s/it]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import time\n",
    "from typing import Optional\n",
    "from IPython.display import display, HTML\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "N_DATA = 100\n",
    "\n",
    "RAW_FILE = \"synth_raw_file_llama_round_2.txt\"\n",
    "JSON_FILE = \"synth_conversations_llama_round_2.json\"\n",
    "\n",
    "\n",
    "def random_selection(elements: list, proba: float) -> Optional[str]:\n",
    "    cond = [proba/len(elements) for _ in range(len(elements))]\n",
    "    el = [None]\n",
    "    el.extend(elements)\n",
    "    weights = [1 - proba]\n",
    "    weights.extend(cond)\n",
    "    return random.choices(el, weights=weights, k=1)[0]\n",
    "\n",
    "\n",
    "\n",
    "for N_position in tqdm(range(N_DATA), desc=\"SYNTH DATA $ 💉 COOKING ☕️\", total=N_DATA, ncols=110):\n",
    "    persona_applicant = random.choice(__persona_applicant)\n",
    "\n",
    "    p_type_applicant, probabilities_list = get_personality_indicator(persona_applicant)\n",
    "\n",
    "    persona_applicant_posts = random.choice([ p['posts'] for p in  post_list if p['type'] == p_type_applicant])\n",
    "\n",
    "    persona_recruitment = random.choice(__persona_recruitment)\n",
    "\n",
    "    p_type_recruitment, probabilities_list = get_personality_indicator(persona_recruitment)\n",
    "\n",
    "    persona_recruitment_posts = random.choice([ p['posts'] for p in  post_list if p['type'] == p_type_recruitment])\n",
    "\n",
    "    recruitment_posts = \"\\n\".join(random.choices(list(persona_recruitment_posts), k=10))\n",
    "    applicant_posts = \"\\n\".join(random.choices(list(persona_applicant_posts), k=10))\n",
    "\n",
    "\n",
    "    # display(HTML(f\"<h3>Applicant: {persona_applicant}</h3>\\n<h2>Personality: {p_type_applicant}</h3>\"))\n",
    "    # display(HTML(f\"<h3>Recuiter: {persona_recruitment}</h3>\\n<h2>Personality: {p_type_recruitment}</h3>\"))\n",
    "\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    Generate a conversational chitchat dialogue between an applicant and a recruiter (Maria).\n",
    "    All conversation is conducted through WhatsApp.\n",
    "    The position to be filled is as an agent in a call center.\n",
    "\n",
    "    Take into account the Personas and Myers–Briggs personality types indicator to enrich and make the dialogue more realistic:\n",
    "    - Applicant Persona: {persona_applicant}\n",
    "    - Applicant Indicator: {p_type_applicant}\n",
    "    - Recruiter Persona: {persona_recruitment}\n",
    "    - Recruiter Indicator: {p_type_recruitment}\n",
    "\n",
    "    Follow the guidances:\n",
    "    - The applicant initiates the conversation with a short greeting, it is possible that the applicant did not know the name of the recruiter.\n",
    "    - The applicant uses informal language throughout the conversation and slangs.\n",
    "    - The recruiter's goal is to be proactive and guide the applicant through a series of tasks.\n",
    "    - The text includes metadata enclosed in pipe, You should NOT explicitly put this metadata in the generation.\n",
    "    - Craft a realistic and engaging conversational dialogue that simulates a real-world recruitment chat.\n",
    "    - Ensure the recruiter provides clear guidance and feedback between each task.\n",
    "    - Aim to reflect the authentic dynamics of a recruitment process, highlighting both the recruiter's professional advice and the applicant's genuine responses.\n",
    "    - No need to reiterate the information in the chat, as it's already captured in the submitted form. Let's move forward with the next steps!\n",
    "    - There is no need to transcribe voice notes in chat.\n",
    "    - Recruiter should be proactive.\n",
    "    - Recruiter should push the applicant to complete each step.\n",
    "    - In a new turn the applicant must explicitly indicate whether they completed a step or after sending a voice note.\n",
    "\n",
    "    Tasks to be completed during the chat:\n",
    "    1. Complete a basic form with required information, As a proactive recruiter, share the form.  |This step consists of the name, English level and accepting the terms and conditions. This form is shared within the chat|\n",
    "    2. Answer a two-question assessment within a form to evaluate your skills, it is not necessary to write the questions in the chat as they are included in the form,  As a proactive recruiter, share the form. |The applicant must answer two questions in two long paragraphs|\n",
    "    3. Record a voice note reading a provided text aloud and send it to the recruiter, As a proactive recruiter, share the text. |Write `PLACEHOLDER_1` intead of text, The text to be read is random and challenges the applicant's level of English.|\n",
    "    4. Respond to an open-ended question with a voice note,  allowing the recruiter to assess their thoughts and opinion, as a proactive recruiter, share the open-ended question. |Write `PLACEHOLDER_2` intead of text.|\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # gpt4o_output = gpt4o(prompt=prompt, temperature=0.7)\n",
    "    # gpt4o_output = \" \".join([out for out in gpt4o_output])\n",
    "\n",
    "    llama3_output = llama3(prompt=prompt, temperature=0.3)\n",
    "    llama3_output = \" \".join([out for out in llama3_output])\n",
    "\n",
    "    time.sleep(7)\n",
    "\n",
    "    # print(\"----------------------------------------      SEAMLESS CONVERSATION      ------------------------------------------\")\n",
    "    # print(llama3_output)\n",
    "    # print(\"--------------------------------------------------------------------------------------------------------------------\\n\\n\\n\")\n",
    "\n",
    "\n",
    "    # proba 1.0\n",
    "    system_task_completed = \"Regardless of the conversation flow, for each completed task there is a system notification with a message like this: ***System***: Task N completed, for a scheduled task: ***System***: Scheduled by the user.\"\n",
    "    # proba 0.7\n",
    "    questions_and_concerns = \"Between turns, incorporate the applicant's questions, concerns, opinions, doubts, statements, and any other dialogue that may naturally arise during the interaction, the Recruiter should assist the applicant in the new incorporated questions, concerns, opinions, doubts, statements, and any other dialogue that may naturally arise during the interaction.\"\n",
    "\n",
    "    # proba 0.3\n",
    "    busy_time_1 = \"Restriction: NOT after last step have had completed, only one time between turns, the applicant expresses that they can't continue right now, are busy, or prefer to continue later. They might suggest any alternative way to stop the process and continue later. The Recruiter should assist the applicant by providing a form to schedule a time when the applicant can continue with the process. After the scheduling, simulate a time-elepsed and continue the conversation.\"\n",
    "    busy_time_2 = \"Restriction: NOT after last step have had completed, only one time between turns, the applicant expresses in an alternative way: 'It is very loud here, can we skip it for now?'.  The Recruiter should assist the applicant by providing a form to schedule a time when the applicant can continue with the process. After the scheduling, simulate a time-elepsed and continue the conversation.\"\n",
    "    busy_time_3 = \"Restriction: NOT after last step have had completed, only one time between turns, the applicant expresses in an alternative way: 'I am driving and I cant read. I will do it later.'.  The Recruiter should assist the applicant by providing a form to schedule a time when the applicant can continue with the process. After the scheduling, simulate a time-elepsed and continue the conversation.\"\n",
    "    busy_time_4 = \"Restriction: NOT after last step have had completed, only one time between turns, the applicant expresses in an alternative way: 'I prefer to continue in person. I am busy now.'.  The Recruiter should assist the applicant by saying that this process needs to be complete by this chat because is quickly and convenient for the applicant, also providing a form to schedule a time when the applicant can continue with the process. After the scheduling, simulate a time-elepsed and continue the conversation.\"\n",
    "    busy_time_5 = \"Restriction: NOT after last step have had completed, only one time between turns, the applicant expresses in an alternative way: 'I am sorry, I forgot. I'll get back to you when I'm ready to continue.'.  The Recruiter should assist the applicant by providing a form to schedule a time when the applicant can continue with the process. After the scheduling, simulate a time-elepsed and continue the conversation.\"\n",
    "\n",
    "    # proba 0.3\n",
    "    ask_for_missing_form = \"Restriction: The Applicant cannot ask for a task that has already been completed, Sometimes the Applicant ask for resend of any specific form, task or question, reason for resend: the applicant may have missing it in the chat.\"\n",
    "\n",
    "    # proba 0.2\n",
    "    not_interested_1 = \"In the middle of the dialogue, the applicant expresses that they don't want to continue or they are not interested anymore. The Recruiter should try to convince him or her that this is his or her best option. Also should assist the applicant by providing a feedback and sharing a form to schedule a time if the applicant change their mind to continue with the process. Only if the applicant does not want to continue and the process ends.\"\n",
    "    not_for_me = \"Only once, after completing a given task, the applicant expresses in an alternative way: 'I don't think this is for me.'. The Recruiter should try to convince him or her that this is his or her best option. Also should assist the applicant and motivate the applicant to continue by saying that this process is easy, quickly also convenient for the applicant.\"\n",
    "\n",
    "    # proba 0.3\n",
    "    not_interested_2_after_scheduling = \"After the scheduling the process, the applicant expresses in an alternative way: 'I am sorry, I recently got a job. Thank you anyway.' in the form that they are not interested anymore. The Recruiter should assist the applicant by providing a feedback and sharing a form to schedule a time if the applicant change their mind to continue with the process. Only if the applicant does not want to continue and the process ends.\"\n",
    "    not_interested_3_after_scheduling = \"After the scheduling the process, the applicant expresses in an alternative way: 'thank you for following up, but I changed my mind.' in the form that they are not interested anymore. The Recruiter should try to convince him or her that this is his or her best option. Also should assist the applicant by providing a feedback and sharing a form to schedule a time if the applicant change their mind to continue with the process. Only if the applicant does not want to continue and the process ends.\"\n",
    "\n",
    "    # proba 0.3\n",
    "    ask_in_person = \"In the middle of the dialogue, at a certain turn, the applicant asks if he or she can do the procedure personally. The Recruiter should assist the applicant by saying that this process needs to be complete by this chat because is quickly and convenient for the applicant.\"\n",
    "    ask_for_different_way = \"In the middle of the dialogue, at a certain turn, the applicant asks if he or she can do this in a different way because he or she is old school. The Recruiter should assist the applicant by saying that this process needs to be complete by this chat because is quickly and convenient for the applicant.\"\n",
    "    ask_for_send_resume = \"In the middle of the dialogue, at a certain turn, the applicant expresses, in an alternative way of say that if he or she can send his resume. The recruiter should assist the applicant by explaining that is convenient to complete the process within a chat  because is very effective and quicker and convenient for the applicant.\"\n",
    "    do_over_the_phone = \"Only once after Recruiter greetings, the applicant expresses, in an alternative way of say that their phone's screen is cracked and he or she can't read very well' with ther porpuse of do the process over the phone. The recruiter should assist the applicant by explaining that is convenient to complete the process within a chat  because is very effective and quicker and convenient for the applicant.\"\n",
    "\n",
    "    # proba 0.3\n",
    "    too_complicated = \"Only once after Recruiter greetings, the applicant expresses in an alternative way: 'Can I just send you my resume? This is too complicated.'. The Recruiter should assist the applicant by saying that this process is easy and needs to be complete by this chat because is quickly and convenient for the applicant.\"\n",
    "    how_long_is = \"In the middle or at the end of the dialogue, the applicant expresses, in an alternative way, 'How long would it take? I have other job offers,' in the form of a clarifying question. The recruiter should assist the applicant by explaining that this process is more effective and quicker, taking days instead of weeks.\"\n",
    "\n",
    "    # proba 0.4\n",
    "    am_hired = \"In the middle or at the end of the dialogue, the applicant expresses, in an alternative way, 'Am I hired?' in the form of receiving immediate feedback. The recruiter should assist the applicant by explaining that first need to be evaluate by a human recruiter and the process is very effective and quicker, taking days instead of weeks.\"\n",
    "    immediate_feedback = \"Only once, after completing a given task, does the applicant request immediate feedback. The recruiter should assist the applicant by explaining that this is being checked and needs to be evaluated by a human recruiter very efficiently and quickly.\"\n",
    "    project_considered_for = \"In the middle or at the end of the dialogue, the applicant expresses, in an alternative way, 'Which project am I being considered for?'. The recruiter should help the applicant by saying that there are many open positions, first need to be evaluate by a human recruiter and the process is very effective and quicker, taking days instead of weeks.\"\n",
    "    remote_position = \"Between turns, incorporate the applicant's questions, concerns, opinions, doubts, statements, and any other dialogue like if there are open remote position, this may naturally arise during the interaction, the Recruiter should assist the applicant in the new incorporated questions, concerns, opinions, doubts, statements, and any other dialogue that may naturally arise during the interaction.\"\n",
    "\n",
    "    # proba 0.5\n",
    "    ask_for_bonus = \"In the middle or at the end of the dialogue, the applicant expresses, in an alternative way of say: 'Do you pay hiring bonus?'. The recruiter should help the applicant by saying yes, but the details must be in person with a human recruiter.\"\n",
    "    salary_comission = \"In the middle of the dialog, the applicant expresses, in an alternative way of say: 'What's the salary and commission like?'. The recruiter should help the applicant by saying that there are good salary and commission, but the details must be in person with a human recruiter.\"\n",
    "\n",
    "    # proba 0.3\n",
    "    work_here_in_past = \"Only once after Recruiter greetings, the applicant expresses, in an alternative way of say: 'I worked there in the past, do I have to do this process?'. The recruiter should assist the applicant by explaining that this process is very effective and quicker, and asks to please complete it.\"\n",
    "    i_have_good_english_but = \"Only once, after completing a given task, the applicant expresses in an alternative way of say: 'If you see typos it's because I am not wearing my glasses. I have good english.' as an excuse. The recruiter should assist the applicant by explaining that this is being checked and needs to be evaluated by a human recruiter, dont worry this is efficiently and quickly.\"\n",
    "\n",
    "\n",
    "    questions_and_concerns_ = random_selection([questions_and_concerns], 0.5)\n",
    "    busy_time_ = random_selection([busy_time_1, busy_time_2, busy_time_3, busy_time_4, busy_time_5], 0.1)\n",
    "    ask_for_missing_form_ = random_selection([ask_for_missing_form], 0.2)\n",
    "    not_interested__ = random_selection([not_interested_1, not_for_me], 0.05)\n",
    "    not_interested_after_scheduling_ = random_selection([not_interested_2_after_scheduling, not_interested_3_after_scheduling], 0.05)\n",
    "    ask_in_person__ = random_selection([ask_in_person, ask_for_different_way, ask_for_send_resume, do_over_the_phone], 0.1)\n",
    "    complicated__ = random_selection([too_complicated, how_long_is], 0.1)\n",
    "    feedback__ = random_selection([am_hired,  immediate_feedback, project_considered_for, remote_position], 0.2)\n",
    "    salary_and_bonus_ = random_selection([ask_for_bonus, salary_comission], 0.2)\n",
    "    another_ = random_selection([work_here_in_past, i_have_good_english_but], 0.05)\n",
    "\n",
    "    concerns = [questions_and_concerns_, busy_time_, ask_for_missing_form_, not_interested__, not_interested_after_scheduling_, ask_in_person__, complicated__, feedback__, salary_and_bonus_, another_]\n",
    "    random.shuffle(concerns)\n",
    "\n",
    "\n",
    "    new_prompt = \"\"\"Given the following WhatsApp Chat Conversation enclosed in a triple backtick, to simulates a real-world recruitment chat perform the following:\\n\"\"\"\n",
    "\n",
    "    #llama3\n",
    "    new_prompt += \"\"\"MODIFY THE GIVEN CONVERSATION INSERTING BETWEEN TURNS SOME DIALOGS ACCORDING TO THE FOLLOWING RULES:\\n\"\"\"\n",
    "    new_prompt += \"\"\"- INSERT AFTER A COMPLETED TASK A MESSAGE LIKE THIS: ***SYSTEM***: TASK N COMPLETED, FOR A SCHEDULED TASK: ***SYSTEM***: SCHEDULED BY THE USER.\\n\"\"\"\n",
    "\n",
    "    for concern in concerns:\n",
    "        if concern:\n",
    "            new_prompt += f\"\"\"- {concern}\\n\"\"\"\n",
    "\n",
    "    new_prompt += f\"\"\"- {system_task_completed}\\n\\n\"\"\"\n",
    "    new_prompt += f\"\"\"```{llama3_output}```\"\"\"\n",
    "\n",
    "    \n",
    "\n",
    "    # print(\"------------------------------------------     PROMPT GENERATED       --------------------------------------------\")\n",
    "    # print(new_prompt)\n",
    "    # print(\"--------------------------------------------------------------------------------------------------------------------\\n\\n\\n\")\n",
    "\n",
    "\n",
    "    new_prompt_ouput = llama3(prompt=new_prompt, temperature=0.2)\n",
    "    new_prompt_ouput = \" \".join([out for out in new_prompt_ouput])\n",
    "\n",
    "    # print(\"------------------------------------------    MODIFIED CONVERSATION    --------------------------------------------\")\n",
    "    # print(new_prompt_ouput)\n",
    "    # print(\"--------------------------------------------------------------------------------------------------------------------\\n\\n\\n\")\n",
    "    \n",
    "    \n",
    "    new_prompt_ouput += \"\\n\\n\\n\"\n",
    "    with open(RAW_FILE, \"a\") as raw:\n",
    "        raw.write(new_prompt_ouput)\n",
    "\n",
    "    \n",
    "    time.sleep(7)\n",
    "\n",
    "\n",
    "    format_instruct = '''GIVEN THE FOLLOWING WHATSAPP CHAT ENCLOSED IN TRIPLE BACKTICK CONVERSATION FORMAT AS JSON AS FOLLOW AND RETURN ONLY THE JSON, DO NOT INCLUDE EXTRA INFORMATION LIKE 'Here is the JSON representation of the ...':\n",
    "    {\n",
    "       \"conversation_''' + str(N_position) + '''\": [\n",
    "            {\n",
    "            \"speaker\": \"Applicant\",\n",
    "            \"message\": \"String ... \"\n",
    "            },\n",
    "            {\n",
    "            \"speaker\": \"Recruiter\",\n",
    "            \"message\": \"String ...\"\n",
    "            },\n",
    "            {\n",
    "            \"speaker\": \"Applicant\",\n",
    "            \"message\": \"String ...\"\n",
    "            },\n",
    "            {\n",
    "            \"speaker\": \"Recruiter\",\n",
    "            \"message\": \"String ...\"\n",
    "            },\n",
    "            {\n",
    "            \"speaker\": \"Applicant\",\n",
    "            \"message\": \"String ...\"\n",
    "            },\n",
    "            {\n",
    "            \"speaker\": \"System\",\n",
    "            \"message\": \"String ...\"\n",
    "            },\n",
    "       ]\n",
    "    },\n",
    "    '''\n",
    "\n",
    "    format_instruct += f\"\\n\\n ```{new_prompt_ouput}```\"\n",
    "\n",
    "    format_output = llama3(prompt=format_instruct, temperature=0.0)\n",
    "    format_output = \" \".join([out for out in format_output])\n",
    "\n",
    "    with open(JSON_FILE, \"a\") as raw:\n",
    "        raw.write(format_output)\n",
    "\n",
    "\n",
    "    time.sleep(7)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488e32ec",
   "metadata": {},
   "source": [
    "img hidde here\n",
    "<!-- <img src=\"https://upload.wikimedia.org/wikipedia/commons/1/1f/MyersBriggsTypes.png\" alt=\"drawing\" width=\"700\"/> -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ddb8933",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4298fcb4",
   "metadata": {},
   "source": [
    "### Extracting TAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "e319d568",
   "metadata": {},
   "outputs": [],
   "source": [
    "llama3 = dspy.GROQ(\n",
    "    model='llama3-70b-8192',\n",
    "    api_key=secret['GROQ_API_KEY'],\n",
    "    max_tokens=4096*2,\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "b26d670b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GENERATING TAGS ☕️:  16%|███████▋                                        | 16/100 [37:00<3:07:08, 133.67s/it]INFO:backoff:Backing off request(...) for 0.1s (groq.InternalServerError: Error code: 503 - {'error': {'message': 'Service Unavailable', 'type': 'internal_server_error'}})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.1 seconds after 1 tries calling function <function GroqLM.request at 0x1747d9f30> with kwargs {'temperature': 0.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:backoff:Backing off request(...) for 1.8s (groq.InternalServerError: Error code: 503 - {'error': {'message': 'Service Unavailable', 'type': 'internal_server_error'}})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 1.8 seconds after 2 tries calling function <function GroqLM.request at 0x1747d9f30> with kwargs {'temperature': 0.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:backoff:Backing off request(...) for 0.3s (groq.InternalServerError: Error code: 503 - {'error': {'message': 'Service Unavailable', 'type': 'internal_server_error'}})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.3 seconds after 3 tries calling function <function GroqLM.request at 0x1747d9f30> with kwargs {'temperature': 0.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:backoff:Backing off request(...) for 1.3s (groq.InternalServerError: Error code: 503 - {'error': {'message': 'Service Unavailable', 'type': 'internal_server_error'}})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 1.3 seconds after 4 tries calling function <function GroqLM.request at 0x1747d9f30> with kwargs {'temperature': 0.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:backoff:Backing off request(...) for 12.2s (groq.InternalServerError: Error code: 503 - {'error': {'message': 'Service Unavailable', 'type': 'internal_server_error'}})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 12.2 seconds after 5 tries calling function <function GroqLM.request at 0x1747d9f30> with kwargs {'temperature': 0.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:backoff:Backing off request(...) for 2.8s (groq.InternalServerError: Error code: 503 - {'error': {'message': 'Service Unavailable', 'type': 'internal_server_error'}})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 2.8 seconds after 6 tries calling function <function GroqLM.request at 0x1747d9f30> with kwargs {'temperature': 0.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:backoff:Backing off request(...) for 50.2s (groq.InternalServerError: Error code: 503 - {'error': {'message': 'Service Unavailable', 'type': 'internal_server_error'}})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 50.2 seconds after 7 tries calling function <function GroqLM.request at 0x1747d9f30> with kwargs {'temperature': 0.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:backoff:Backing off request(...) for 87.6s (groq.InternalServerError: Error code: 503 - {'error': {'message': 'Service Unavailable', 'type': 'internal_server_error'}})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 87.6 seconds after 8 tries calling function <function GroqLM.request at 0x1747d9f30> with kwargs {'temperature': 0.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:backoff:Backing off request(...) for 0.2s (groq.InternalServerError: Error code: 503 - {'error': {'message': 'Service Unavailable', 'type': 'internal_server_error'}})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.2 seconds after 1 tries calling function <function GroqLM.request at 0x1747d9f30> with kwargs {'temperature': 0.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:backoff:Backing off request(...) for 1.6s (groq.InternalServerError: Error code: 503 - {'error': {'message': 'Service Unavailable', 'type': 'internal_server_error'}})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 1.6 seconds after 2 tries calling function <function GroqLM.request at 0x1747d9f30> with kwargs {'temperature': 0.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:backoff:Backing off request(...) for 3.9s (groq.InternalServerError: Error code: 503 - {'error': {'message': 'Service Unavailable', 'type': 'internal_server_error'}})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 3.9 seconds after 3 tries calling function <function GroqLM.request at 0x1747d9f30> with kwargs {'temperature': 0.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:backoff:Backing off request(...) for 2.9s (groq.InternalServerError: Error code: 503 - {'error': {'message': 'Service Unavailable', 'type': 'internal_server_error'}})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 2.9 seconds after 4 tries calling function <function GroqLM.request at 0x1747d9f30> with kwargs {'temperature': 0.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:backoff:Backing off request(...) for 4.4s (groq.InternalServerError: Error code: 503 - {'error': {'message': 'Service Unavailable', 'type': 'internal_server_error'}})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 4.4 seconds after 5 tries calling function <function GroqLM.request at 0x1747d9f30> with kwargs {'temperature': 0.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GENERATING TAGS ☕️: 100%|███████████████████████████████████████████████| 100/100 [3:57:24<00:00, 142.44s/it]\n"
     ]
    }
   ],
   "source": [
    "import json \n",
    "import random\n",
    "import time\n",
    "\n",
    "with open('synth_data/synth_conversations_llama_round_2.json', \"r\") as synth:\n",
    "    conversations = json.load(synth)\n",
    "\n",
    "\n",
    "\n",
    "TAGS = \"\"\"# Task Description\n",
    "\n",
    "Your task is to condense a message from a recruitment service dialog into a concise word. \n",
    "\n",
    "To achieve this, follow these guidelines:\n",
    "\n",
    "1. **Brevity is key**: Limit your summary to 3 words or less, focusing on the essential information.\n",
    "2. **Event-centric**: Extract the main event, issue, query, solution, or feedback from the message.\n",
    "\n",
    "\n",
    "\n",
    "# Example\n",
    "\n",
    "Input: \"Oh, I forgot to ask, could you resend the assessment form link? I might have missed something.\"\n",
    "\n",
    "Output: \"#resend #assessment\"\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "with_tags = []\n",
    "for i in tqdm(range(len(conversations)), desc=\"GENERATING TAGS ☕️\", total=len(conversations), ncols=110):\n",
    "  conversation = conversations[i]\n",
    "  for k, value in conversation.items():\n",
    "    current = conversation[k]\n",
    "    # TAGS = TAGS.format(\"\\n\".join([f\"{d['speaker']}: {d['message']}\" for d in current]))\n",
    "    for j, v in enumerate(value):\n",
    "      message = v['message']\n",
    "      TAGS = f\"\"\"\n",
    "GIVEN THE FOLLOWING DIALOGUE TURN ENCLOSED IN TRIPLE BACKTICKS:\n",
    "```{message}```\n",
    "\n",
    "\n",
    "PERFORM THE FOLLOWING TASKS:\n",
    "\n",
    "Condense the message into a concise hashtag, such as \"#interest\" or \"#position\".\n",
    "Extract the main event, issue, query, solution, or feedback from the message.\n",
    "THE OUTPUT SHOULD BE 1 TO 3 WORDS, E.G., \"#introduction\" OR \"#callcenter\".\n",
    "DO NOT INCLUDE ANY ADDITIONAL TEXT BEYOND THE FINAL HASHTAGS.\n",
    "\n",
    "\n",
    "TAKE INTO ACCOUNT THE COMPLETE CONVERSATION AS A CONTEXT:\n",
    "```{\" \".join([f\"{d['speaker']}: {d['message']}\" for d in current])}```\n",
    "            \"\"\"\n",
    "      \n",
    "      tags_output = llama3(prompt=TAGS, temperature=0.2)\n",
    "      tags_output = \" \".join([out for out in tags_output])\n",
    "      current[j]['tags'] = tags_output\n",
    "\n",
    "      time.sleep(random.randint(1, 3))\n",
    "\n",
    "    with open(\"synth_conversations_llama_round_2_with_tags.json\", \"w\") as raw:\n",
    "      with_tags.append(current)\n",
    "      json.dump(with_tags, raw, indent=4)\n",
    "\n",
    "\n",
    "# print(conversations[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eceeeea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
